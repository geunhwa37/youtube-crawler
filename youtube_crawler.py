import os
import json
import re
import pandas as pd
from googleapiclient.discovery import build
from datetime import datetime, timedelta
import yt_dlp
from faster_whisper import WhisperModel
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from gspread_dataframe import set_with_dataframe

# âœ… Secrets
API_KEY = os.environ["YOUTUBE_API_KEY"]
GSHEETS_KEY = os.environ["GSHEETS_KEY"]

# ğŸ“Œ ê´‘ê³  í‚¤ì›Œë“œ
ads_keywords = [
    "í• ì¸","ì´ë²¤íŠ¸","íŠ¹ê°€","ë¬´ë£Œê²€ì§„","ë³´í—˜","ë¹„ê¸‰ì—¬","ì‹¤ì†",
    "ìƒë‹´","ë¬¸ì˜","í™•ì‹¤","ë³´ì¥","ì˜ˆì•½","ì €ë ´","ë¬´ë£Œ","í˜œíƒ"
]

# ğŸ“Œ ìœ„í—˜ í‚¤ì›Œë“œ
risk_keywords = [
    "ì¤„ê¸°ì„¸í¬","ë¬´ë¦ì¤„ê¸°ì„¸í¬ì£¼ì‚¬","ì—¬ìœ ì¦","ë„ìˆ˜ì¹˜ë£Œ","ë¹„ê¸‰ì—¬ì£¼ì‚¬",
    "ë§˜ëª¨í†°","ë°œë‹¬ì§€ì—°","ìš”ì–‘í•œë°©ë³‘ì›","ë¬´ë¦ê´€ì ˆì¦","í•˜ì§€ì •ë§¥ë¥˜",
    "ê°‘ìƒì„ ê²°ì ˆ","ì•¡ì·¨ì¦"
]

# ğŸ™ Whisper ëª¨ë¸ (CPU)
model = WhisperModel("base", device="cpu", compute_type="int8")

# ğŸ“Œ êµ¬ê¸€ì‹œíŠ¸ ì—°ê²° (ID ë²„ì „)
def connect_gsheet():
    creds_dict = json.loads(GSHEETS_KEY)
    scope = ["https://spreadsheets.google.com/feeds",
             "https://www.googleapis.com/auth/drive"]
    creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
    client = gspread.authorize(creds)
    # ğŸ”‘ íŒŒì¼ IDë¡œ ì—´ê¸°
    return client.open_by_key("11N-GVX670-a1-pwsA7Qs0o9HwqBgiJOHMgJ7Me-IKjs").worksheet("STTë³€í™˜ê²°ê³¼")


# ğŸ“Œ ë°ì´í„° ì‹œíŠ¸ ì—…ë¡œë“œ
def upload_to_sheet(df, sheet):
    existing = sheet.get_all_values()
    start_row = len(existing) + 1
    set_with_dataframe(sheet, df, row=start_row, include_column_header=(start_row == 1))
    print(f"âœ… êµ¬ê¸€ì‹œíŠ¸ ì—…ë¡œë“œ ì™„ë£Œ ({len(df)} í–‰ ì¶”ê°€ë¨)")

# ğŸ“Œ ìœ íŠœë¸Œ í¬ë¡¤ë§ (ì¡°íšŒìˆ˜ í¬í•¨)
def crawl_youtube_videos(keyword, published_after, max_results=20):
    youtube = build("youtube", "v3", developerKey=API_KEY)
    search_response = youtube.search().list(
        q=keyword,
        part="id,snippet",
        type="video",
        order="date",
        publishedAfter=published_after,
        maxResults=max_results
    ).execute()

    videos = []
    for item in search_response.get("items", []):
        video_id = item["id"]["videoId"]

        # ì¡°íšŒìˆ˜ í¬í•¨ ìœ„í•´ videos.list í˜¸ì¶œ
        video_response = youtube.videos().list(
            part="snippet,statistics",
            id=video_id
        ).execute()

        if not video_response["items"]:
            continue

        v = video_response["items"][0]
        snippet = v["snippet"]
        stats = v.get("statistics", {})

        videos.append({
            "ê²€ìƒ‰ í‚¤ì›Œë“œ": keyword,
            "ë¹„ë””ì˜¤ ID": video_id,
            "ì œëª©": snippet["title"],
            "ì„¤ëª…": snippet.get("description", ""),
            "ì±„ë„ëª…": snippet["channelTitle"],
            "ì˜¬ë¦° ë‚ ì§œ": pd.to_datetime(snippet["publishedAt"]).strftime("%Y-%m-%d"),
            "ì¡°íšŒìˆ˜": int(stats.get("viewCount", 0)),
            "URL": f"https://www.youtube.com/watch?v={video_id}",
            "ê´‘ê³ ì„± í‘œí˜„ (T/F)": any(ad in (snippet["title"] + snippet.get("description","")) for ad in ads_keywords)
        })
    return videos


# ğŸ“Œ STT ë³€í™˜
def transcribe_video(video_id):
    video_url = f"https://www.youtube.com/watch?v={video_id}"
    ydl_opts = {
        "format": "bestaudio/best",
        "outtmpl": f"/tmp/{video_id}.%(ext)s",
        "cookiefile": "cookies.txt",   # âœ… ì¶”ê°€
        "postprocessors": [{
            "key": "FFmpegExtractAudio",
            "preferredcodec": "mp3",
            "preferredquality": "192",
        }],
        "quiet": True
    }
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            ydl.download([video_url])
        audio_file = f"/tmp/{video_id}.mp3"
        if not os.path.exists(audio_file):
            return "âš ï¸ ì˜¤ë””ì˜¤ ì—†ìŒ"
        segments, _ = model.transcribe(audio_file, language="ko")
        text = " ".join([seg.text for seg in segments])
        # ê°„ë‹¨ êµì •
        corrections = {"ì£¼ê¸°ì„¸í¬":"ì¤„ê¸°ì„¸í¬","ë„ìˆ˜ì¹˜ë£Œë²•":"ë„ìˆ˜ì¹˜ë£Œ"}
        for wrong, correct in corrections.items():
            text = text.replace(wrong, correct)
        return re.sub(r'\s+', ' ', text).strip()
    except Exception as e:
        return f"âš ï¸ ì˜¤ë¥˜: {str(e)}"

# ğŸ“Œ ë©”ì¸
def main():
    today = datetime.utcnow().date()
    yesterday = today - timedelta(days=1)
    published_after = datetime.combine(yesterday, datetime.min.time()).isoformat("T") + "Z"

    final_data = []
    for kw in risk_keywords:
        videos = crawl_youtube_videos(kw, published_after)
        if not videos: continue
        df = pd.DataFrame(videos)

        # ê´‘ê³  í¬í•¨ë§Œ í•„í„°
        df = df[df.apply(lambda row: any(ad in (row["title"]+row["description"]) for ad in ads_keywords), axis=1)]
        if df.empty: continue

        # STT
        df["transcript"] = df["video_id"].apply(transcribe_video)
        final_data.append(df)

    if final_data:
        df_final = pd.concat(final_data, ignore_index=True)
    
        # ì»¬ëŸ¼ ìˆœì„œ ê³ ì •
        df_final = df_final[
            ["ê²€ìƒ‰ í‚¤ì›Œë“œ","ë¹„ë””ì˜¤ ID","ì œëª©","ì„¤ëª…","ì±„ë„ëª…",
             "ì˜¬ë¦° ë‚ ì§œ","ì¡°íšŒìˆ˜","URL","ê´‘ê³ ì„± í‘œí˜„ (T/F)"]
        ]
    
        sheet = connect_gsheet()
        upload_to_sheet(df_final, sheet)
    else:
        print("âŒ ì˜¤ëŠ˜ì€ ë°ì´í„° ì—†ìŒ")

if __name__ == "__main__":
    main()




